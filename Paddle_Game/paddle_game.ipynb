import parl
import numpy as np
from parl.utils import logger
from parl.algorithms import DQN

from agent import Agent
from model import Model
from replay_memory import ReplayMemory
import train
from Paddle import Paddle

MEMORY_SIZE = 20000    # replay memory的大小，越大越占用内存
MEMORY_WARMUP_SIZE = 200  # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn
GAMMA = 0.95 # reward 的衰减因子，一般取 0.9 到 0.999 不等
LEARNING_RATE = 0.001 # 学习率

# 创建环境
env = Paddle()
action_dim = 3  #向左=0 不动=1 向右=2
obs_shape = 5   #5个state


# 创建经验池
rpm = ReplayMemory(MEMORY_SIZE)  # DQN的经验回放池



# 根据parl框架构建agent
# 4. 请参考课堂Demo，嵌套Model, DQN, Agent构建 agent
model = Model(act_dim=action_dim)
algorithm = DQN(model, act_dim=action_dim, gamma=GAMMA, lr=LEARNING_RATE)
agent = Agent(algorithm,obs_dim=obs_shape,act_dim=action_dim,e_greed=0.1,e_greed_decrement=1e-6) 




# 先往经验池里存一些数据，避免最开始训练的时候样本丰富度不够
while len(rpm) < MEMORY_WARMUP_SIZE:
    train.run_episode(env, agent, rpm)

max_episode = 1000

# 开始训练
episode = 0
while episode < max_episode:  # 训练max_episode个回合，test部分不计算入episode数量
    # train part
    for i in range(0, 50):
        total_reward = train.run_episode(env, agent, rpm)
        episode += 1

    # test part
    eval_reward = train.evaluate(env, agent)  # render=True 查看显示效果
    logger.info('episode:{}    e_greed:{}   test_reward:{}'.format(
        episode, agent.e_greed, eval_reward))

    ckpt = './episode2_{}.ckpt'.format(episode)
    agent.save(ckpt)
    
    
# 加载模型
#save_path = './episode_700.ckpt'
#agent.restore(save_path)

#evaluate_reward = train.evaluate(env, agent)
#logger.info('Evaluate reward: {}'.format(evaluate_reward)) # 打印评估的reward
